{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# In a new Colab cell\n",
        "import os\n",
        "import torch\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# --- Configuration ---\n",
        "PDF_SOURCE_DIRECTORY = \"/content/drive/MyDrive/judgments-data\"\n",
        "PERSIST_DIRECTORY = \"vector_db\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "def create_vector_database():\n",
        "    \"\"\"\n",
        "    Creates and persists a vector database from PDF documents in Google Colab.\n",
        "    \"\"\"\n",
        "    print(\"--- ðŸš€ Starting Vector Database Creation ---\")\n",
        "\n",
        "    # 1. Load Documents\n",
        "    print(f\"Loading documents from '{PDF_SOURCE_DIRECTORY}'...\")\n",
        "    loader = DirectoryLoader(\n",
        "        PDF_SOURCE_DIRECTORY,\n",
        "        glob=\"*.pdf\",\n",
        "        loader_cls=PyPDFLoader,\n",
        "        show_progress=True,\n",
        "        use_multithreading=True\n",
        "    )\n",
        "    documents = loader.load()\n",
        "    if not documents:\n",
        "        print(\"No documents found. Please check you've uploaded PDFs to the correct folder.\")\n",
        "        return\n",
        "    print(f\"âœ… Loaded {len(documents)} document(s).\")\n",
        "\n",
        "    # 2. Chunk Documents\n",
        "    print(\"Splitting documents into chunks...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"âœ… Split documents into {len(texts)} chunks.\")\n",
        "\n",
        "    # 3. Initialize Embedding Model (with GPU support)\n",
        "    print(f\"Initializing embedding model: '{EMBEDDING_MODEL_NAME}'...\")\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        model_kwargs={'device': device}\n",
        "    )\n",
        "    print(\"âœ… Embedding model loaded.\")\n",
        "\n",
        "    # 4. Create and Persist Vector Database\n",
        "    print(f\"Creating and persisting vector database in '{PERSIST_DIRECTORY}'...\")\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=texts,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=PERSIST_DIRECTORY\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- âœ… Vector Database Creation Complete! ---\")\n",
        "    print(f\"Database stored in: {PERSIST_DIRECTORY}\")\n",
        "\n",
        "# Run the function\n",
        "create_vector_database()"
      ],
      "metadata": {
        "id": "wcNsGXSxOt2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In a new Colab cell\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import torch\n",
        "\n",
        "# --- Configuration ---\n",
        "PERSIST_DIRECTORY = \"vector_db\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "# 1. Initialize Embeddings and Load Database\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': device})\n",
        "vectordb = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)\n",
        "\n",
        "# 2. Define your query and search\n",
        "query = \"what are the legal principles for granting injunctions\"\n",
        "results = vectordb.similarity_search(query, k=3) # Get the top 3 most similar chunks\n",
        "\n",
        "# 3. Display results\n",
        "print(f\"\\nTop results for query: '{query}'\\n\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"--- Result {i+1} ---\")\n",
        "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"Content: {doc.page_content[:500]}...\") # Print first 500 characters\n",
        "    print(\"-\" * 20 + \"\\n\")"
      ],
      "metadata": {
        "id": "vVmbZ-4bPXaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In a new Colab cell\n",
        "!zip -r vector_db.zip /content/vector_db"
      ],
      "metadata": {
        "id": "nYNICZoVRlQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5e1kbdy4UYBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}